# -*- coding: utf-8 -*-
"""streamlit checkpoint1.ipynb

Automatically generated by Colab.

Original file is located at
    ('/Users/cyrineelbyr/Downloads/Expresso_churn_dataset (1).csv')
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
from mlxtend.preprocessing import TransactionEncoder
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('/Users/cyrineelbyr/Downloads/Expresso_churn_dataset (1).csv')


df.info()
df.head()

df['REGION'].unique()

df['TENURE'].unique()

df['TOP_PACK'].unique()

df['MRG']

df.isnull().sum()

# Calculate the percentage of missing values
missing_percentage = (df.isnull().sum() / len(df)) * 100

# Display the missing percentage for each column
print("Percentage of missing values per column:")
print(missing_percentage)

# Impute missing values for object columns with the mode
for col in df.select_dtypes(include='object').columns:
    if df[col].isnull().any():
        mode_value = df[col].mode()[0]
        df[col].fillna(mode_value, inplace=True)

# Impute missing values for float columns with the median
for col in df.select_dtypes(include='float64').columns:
    if df[col].isnull().any():
        median_value = df[col].median()
        df[col].fillna(median_value, inplace=True)

print("Missing values after imputation:")
print(df.isnull().sum())

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to the 'REGION' column
df['REGION_encoded'] = label_encoder.fit_transform(df['REGION'])

# Display the original 'REGION' column and the new 'REGION_encoded' column
print(df[['REGION', 'REGION_encoded']].head())
print(df[['REGION', 'REGION_encoded']].tail())

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to the 'REGION' column
df['TENURE_encoded'] = label_encoder.fit_transform(df['TENURE'])

# Display the original 'REGION' column and the new 'REGION_encoded' column
print(df[['TENURE', 'TENURE_encoded']].head())
print(df[['TENURE', 'TENURE_encoded']].tail())

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Apply label encoding to the 'REGION' column
df['PACK_encoded'] = label_encoder.fit_transform(df['TOP_PACK'])

# Display the original 'REGION' column and the new 'REGION_encoded' column
print(df[['TOP_PACK', 'PACK_encoded']].head())
print(df[['TOP_PACK', 'PACK_encoded']].tail())

df.drop(['user_id'], axis=1, inplace=True)

df.drop(['REGION'], axis=1, inplace=True)

df.drop(['TENURE'], axis=1, inplace=True)

df.drop(['TOP_PACK'], axis=1, inplace=True)

# Identify numerical columns
numerical_columns = df.select_dtypes(include=np.number).columns.tolist()

# Print the list of numerical columns
print("Numerical columns in the dataset:")
print(numerical_columns)

df.info()



def detect_outliers_iqr(column):
    """Detect outliers in a numerical column using the IQR method."""
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outlier_indices = column[(column < lower_bound) | (column > upper_bound)].index
    return outlier_indices

# Iterate through numerical columns and detect outliers
outliers = {}
for col in numerical_columns:
    # Skip 'CHURN' column as it is the target variable and not a feature for outlier detection
    if col == 'CHURN':
        continue
    outlier_indices = detect_outliers_iqr(df[col])
    outliers[col] = outlier_indices

# Print the number of detected outliers for each numerical column
print("Number of detected outliers per numerical column (using IQR):")
for col, indices in outliers.items():
    print(f"{col}: {len(indices)}")

# Handle the detected outliers using capping
for col, indices in outliers.items():
    if not indices.empty:
        # Calculate Q1, Q3, and IQR for the column
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        # Calculate lower and upper bounds
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Apply capping: replace values below lower bound with lower bound and values above upper bound with upper bound
        df.loc[indices[df.loc[indices, col] < lower_bound], col] = lower_bound
        df.loc[indices[df.loc[indices, col] > upper_bound], col] = upper_bound

print("Outlier handling (capping) completed.")

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Separate features (X) and target variable (y)
# Drop the original 'user_id' and the target variable 'CHURN'
X = df[['REGULARITY','REGION_encoded','PACK_encoded']]
y = df['CHURN']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Classifier
decision_tree_model = DecisionTreeClassifier(random_state=42)

# Train the model on the training data
decision_tree_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = decision_tree_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
numeric_df = df.select_dtypes(include=['number'])  # only keep numeric columns
correlation_matrix = numeric_df.corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
plt.title('Correlation Matrix of the Dataset')
plt.show()

from sklearn.linear_model import LogisticRegression

# Initialize the Logistic Regression model
logistic_regression_model = LogisticRegression(random_state=42)

# Train the model on the training data
logistic_regression_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred_lr = logistic_regression_model.predict(X_test)

# Evaluate the model
accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)
conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)

print("Logistic Regression Model Evaluation:")
print(f"Accuracy: {accuracy_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1-Score: {f1_lr:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_lr)



# Separate features (X) and target variable (y)
# Drop the original 'user_id' and the target variable 'CHURN'
X = df[['REGULARITY','REGION_encoded','PACK_encoded']]
y = df['CHURN']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Logistic Regression model
logistic_regression_model = LogisticRegression(random_state=42)

# Train the model on the training data
logistic_regression_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred_lr = logistic_regression_model.predict(X_test)

# Evaluate the model
accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)
conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)

print("Logistic Regression Model Evaluation:")
print(f"Accuracy: {accuracy_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1-Score: {f1_lr:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix_lr)

"""# Task
Create a Streamlit application that takes user input for the features 'REGULARITY', 'REGION_encoded', and 'PACK_encoded', uses a pre-trained logistic regression model to predict the target variable, and displays the prediction.

## Save the trained model

### Subtask:
Save the trained logistic regression model to a file so it can be loaded into the Streamlit app.

**Reasoning**:
The subtask is to save the trained logistic regression model. I will use the `joblib` library to save the model to a file.
"""

import joblib

# Define the filename for the model
filename = 'logistic_regression_model.joblib'

# Save the trained logistic regression model to the file
joblib.dump(logistic_regression_model, filename)

print(f"Logistic regression model saved to {filename}")

"""## Create a streamlit app file

### Subtask:
Create a new Python file (`.py`) that will contain the code for the Streamlit application.

**Reasoning**:
Create a new Python file named `app.py` in the current working directory.
"""

# This is an empty file that will contain the Streamlit application code.
# The code for the Streamlit app will be added in the next steps.

"""## Import libraries and load the model

### Subtask:
In the Streamlit app file, import the necessary libraries (streamlit, pandas, scikit-learn) and load the saved logistic regression model.

**Reasoning**:
I need to write the code for the Streamlit application in the `app.py` file. The first steps are to import the necessary libraries and load the pre-trained logistic regression model.
"""

import streamlit as st
import pandas as pd
import joblib

# Load the trained logistic regression model
loaded_model = joblib.load('logistic_regression_model.joblib')

st.title('Customer Churn Prediction')
st.write('Enter the customer details to predict churn.')

# Input fields for features
regularity = st.slider('REGULARITY', 0, 62, 30) # Assuming range based on data exploration
region_encoded = st.slider('REGION_encoded', 0, 14, 7) # Assuming range based on data exploration
pack_encoded = st.slider('PACK_encoded', 0, 80, 40) # Assuming range based on data exploration

# Create a DataFrame from the input
input_data = pd.DataFrame([[regularity, region_encoded, pack_encoded]],
                          columns=['REGULARITY', 'REGION_encoded', 'PACK_encoded'])

# Make prediction
prediction = loaded_model.predict(input_data)

# Display prediction
if prediction[0] == 1:
    st.write('Prediction: The customer is likely to churn.')
else:
    st.write('Prediction: The customer is not likely to churn.')

"""**Reasoning**:
The previous command failed because the `streamlit` module is not installed. I need to install it using pip.


"""


"""**Reasoning**:
Now that `streamlit` is installed, I can proceed with writing the Streamlit application code as intended in the previous failed attempt. This involves importing the necessary libraries and loading the pre-trained model.


"""